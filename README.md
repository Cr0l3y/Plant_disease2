Text translated on translate.google EN. - Text in PT-BR 
#### The files were corrupted and this repository was created without further notice.

<p style="font-family:Arial; font-size:30px; color:#4361F7;">
    <em>EN</em>:
</p>




# Project: Image classification with grapevine leaves: Plant disease.

<p style="font-family:Arial; font-size:20px; color:#43CDF7;">
    Provisional IMG of the project:
</p>

![alt text](Uva.jpg)
###### Font IMG : https://saporedivino.com.br/o-ciclo-de-vida-das-videiras/ 


- ***Using convolutional neural networks***

In the project I tried to document as little as possible so that I could include it in the readme in a general way and without polluting it with too much development and code information.

**Importing Data**

The data was imported using the pathlib library, together with the .glob() method, which allows you to filter files with specific patterns, but without recursive searching.

**Obtaining Folder Names**

The folders variable was declared, where I applied a for loop to obtain the names of the subfolders within the main directory. As an example, we extracted the names of the folders containing images:


        ['BlackMeasles', 'BlackRot', 'HealthyGrapes', 'LeafBlight']


**First Image Processing**

After printing the folder names, I took the first image from the BlackMeasles class with:

        PIL.Image.open(str(blackMeasles[0]))


**Image Feature Extraction**

To analyze the dimensions of the images, the path to the subfolder was defined using the path variable. The images variable was created to store the images in the subfolder. Using:


        + list(path.glob("*.jpg"))


to ensure that images with different extensions were found.
Note: Since the formats "JPG" and "jpg" will be added together, if there are other formats, they should be treated separately.

**Image Verification and Conversion**

I implemented an if structure to ensure that there are images before opening any file. If there are images, the first one is opened and converted to a NumPy array for analysis. This ensures that the process occurs safely, avoiding errors if there are no images in the folder.



the images generated by img_array.shape returned (256, 256, 3), which is 256x256 pixels (width and height) and have three color channels, represented by the value 3. In other words, they are stored as tensors to load in batches (path). With "summary" we had the response where we obtained: Input: Image (256x256x3), Preprocessing Layer: Rescaling, Flatten Layer: Converts the image to a 1D vector (196608), Dense Layer 1: 128 neurons, 25M+ parameters and Dense Layer 2 (Output): 4 neurons, 516 parameters.

**image summary:**


<img src="RPsummary.PNG" alt="RPsummary" width="400">

###### Font IMG: Generated by A.I. - ChatGPT


The dataset was divided into one part for training and another for validation using "tf.keras.utils". After the division, a seed was chosen and the model was created with an input layer, rescaling pixel values, breaking the image as a sequence of numbers, creating a dense/hidden layer, and an output layer, then compiling the model.




After plotting the results, the accuracy was around 0.8. Trying to see if I could improve it even further, I looked for specific layers for neural networks that work with images, testing another technique to optimize accuracy.

I added convolutional layers, now with 32 filters, meaning 32 versions of the same data, using Conv2D with a 3x3 pixel size and reducing dimensionality with MaxPooling2D.

Running the model with 10 epochs, the graph showed values close to 1 up to a certain point. Using summary, I analyzed the model’s layer data and set an epoch limit using a class called "myCallback", which stops training when accuracy reaches 0.93. This was just a test to see how far the model could go.

Now, testing with 50 epochs, the data became unstable over time. To try to improve it, I added Data Augmentation to increase the dataset. After running 50 epochs with this technique and plotting the graph, the results indicated some instability in the neural network training. Additionally, it might suggest that the added features, such as rotation and zoom, were not present in the validation data.

Because of this, I decided to use the InceptionV3 model, which I had previously worked with on another Kaggle project. Project summary: pneumonia X-rays, with 16 validation images and 5216 training images. I plotted the data, split it into 80-20, and used initial_bias for correction. The model included Dense and Dropout layers.

I defined the input shape in a variable:


        input_shape = (256, 256, 3)


I downloaded the InceptionV3 model and set trainable = False so it wouldn't be trainable. I took the base model's output for compilation. This time, I ran 20 epochs, and accuracy improved.

Finally, I saved the results both as raw data and in an optimized format, using "include_optimizer", converting and saving the model with TensorFlow Lite.







---
---
#### Os arquivos foram corrompidos e esse repositorio foi feito sem seguida.

<p style="font-family:Arial; font-size:30px; color:#4361F7;">
    <em>PT-BR</em>:
</p>


# Projeto: Classificação de imagem com folhas de videiras : Doença de plantas.

<p style="font-family:Arial; font-size:20px; color:#43CDF7;">
    IMG provisoria do projeto:
</p>



![alt text](Uva.jpg)
###### Font IMG : https://saporedivino.com.br/o-ciclo-de-vida-das-videiras/ 


- ***Utilizando redes neurais convolucionais***


No projeto tentei documentar menos possivel para colocar no readme de forma geral e sem poluir com muito desenvolvimento e informação do codigo.

**Importação dos Dados**

A importação dos dados foi realizada utilizando a biblioteca pathlib, juntamente com o método .glob(), que permite filtrar arquivos com padrões específicos, mas sem busca recursiva.

**Obtenção dos Nomes das Pastas**

Foi declarada a variável pastas, onde apliquei um loop for para obter os nomes das subpastas dentro do diretório principal. Como exemplo, extraímos os nomes das pastas contendo imagens:

Saída:

        ['BlackMeasles', 'BlackRot', 'HealthyGrapes', 'LeafBlight']


**Processamento da Primeira Imagem**


Após imprimir os nomes das pastas, peguei a primeira imagem da classe BlackMeasles com:

        PIL.Image.open(str(blackMeasles[0]))


**Extração das Características da Imagem**

Para analisar as dimensões das imagens, foi definido o caminho para a subpasta usando a variável path. A variável images foi criada para armazenar as imagens da subpasta. Usando:


        + list(path.glob("*.jpg"))

para garantir que imagens com diferentes extensões fossem encontradas.
Observação: Como os formatos "JPG" e "jpg" serão somados, caso haja outros formatos, eles devem ser tratados separadamente.



**Verificação e Conversão das Imagens**

implementei uma estrutura if para garantir que haja imagens antes de abrir qualquer arquivo. Caso existam imagens, a primeira é aberta e convertida em um array NumPy para análise. Assim, garantindo que o processo ocorra de maneira segura, evitando erros caso não haja imagens na pasta.

as imagens geradas por img_array.shape retornou (256, 256, 3), que é 256x256 pixels (largura e altura) e possuem três canais de cor, representados pelo valor 3. Ou seja são armazeradas como tensores para carregar em lotes (path).Com "summary" tivemos a reposta onde se obteve: Entrada: Imagem (256x256x3), Camada de Pré-processamento: Rescaling, Camada de Flatten: Converte a imagem um vetor 1D (196608), Camada Densa 1: 128 neurônios, 25M+ parâmetros e a Camada Densa 2 (Saída): 4 neurônios, 516 parâmetros.


**summary em imagem:**


<img src="RPsummary.PNG" alt="RPsummary" width="400">

###### Font IMG : Gerada por I.A. - ChatGPT



Foi realizado a divisão dataset uma parte para treino e outro para validação utilizando "tf.keras.utils.". Após a divisão foi feita a escolha de uma seed e a criação do modelo com camada de entrada, reescalando valores dos pixels, quebrando a imagem como uma sequencia de numes, criando uma camada densa/escondida, e camada de saida, em seguida a copilação do modelo













Após plotar os resultados, a acurácia ficou em torno de 0.8. Tentando ver se dava para melhorar ainda mais, fui em busca de camadas específicas para redes neurais que trabalham com imagens, testando outra técnica para otimizar a acurácia.

Adicionei camadas convolucionais, agora com 32 filtros, ou seja, 32 versões do mesmo dado, usando Conv2D com tamanho 3x3 pixels e reduzindo a dimensionalidade com MaxPooling2D.

Rodando o modelo com 10 épocas, o gráfico mostrou valores próximos de 1 até certo ponto. Usando o summary, analisei os dados das camadas do modelo e configurei um limite para as épocas com uma classe chamada "myCallback", que interrompe o treinamento ao atingir 0.93 de acurácia. Isso foi apenas um teste para ver até onde o modelo chegaria.

Agora, testando com 50 épocas, os dados ficaram instáveis ao longo do tempo. Para tentar melhorar, adicionei Data Augmentation para aumentar a base de dados. Depois de rodar as 50 épocas com essa técnica e plotar o gráfico, o resultado indicou certa instabilidade no treinamento da rede neural. Além disso, pode indicar que as características adicionadas, como rotação e zoom, não estavam presentes nos dados de validação.

Diante disso, resolvi utilizar o modelo InceptionV3, que já trabalhei em outro projeto no Kaggle. Resumo do projeto: raios X de pneumonia, com 16 imagens de validação e 5216 imagens de treino. Fiz a plotagem dos dados, dividi em 80-20, e utilizei initial_bias para correção. No modelo, trabalhei com camadas Dense e Dropout.

Defini a forma do input em uma variável:

        input_shape = (256, 256, 3)

Baixei o modelo InceptionV3 e configurei trainable = False para que ele não fosse treinável. Peguei a saída do modelo base para a compilação. Desta vez, rodei 20 épocas e a acurácia melhorou.

Por fim, salvei os resultados tanto em dados brutos quanto no modo otimizado, usando "include_optimizer", convertendo e salvando o modelo com TensorFlow Lite.

